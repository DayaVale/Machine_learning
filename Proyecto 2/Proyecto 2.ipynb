{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d12e003",
   "metadata": {},
   "source": [
    "# Proyecto 2\n",
    " \n",
    "- *Dayana Valentina Gonzalez Vargas*\n",
    "- *Juan Manuel Ramirez*\n",
    "\n",
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "758eebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46608053",
   "metadata": {},
   "source": [
    "### Cargar los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f735cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>X</th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.060495</td>\n",
       "      <td>0.187476</td>\n",
       "      <td>0.126197</td>\n",
       "      <td>0.233586</td>\n",
       "      <td>0.107389</td>\n",
       "      <td>0.869088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.137742</td>\n",
       "      <td>0.023022</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.777344</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>6.226562</td>\n",
       "      <td>6.140625</td>\n",
       "      <td>0.116586</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.062260</td>\n",
       "      <td>0.195070</td>\n",
       "      <td>0.130847</td>\n",
       "      <td>0.243987</td>\n",
       "      <td>0.113140</td>\n",
       "      <td>1.191767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.121811</td>\n",
       "      <td>0.018412</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.930339</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.914062</td>\n",
       "      <td>0.144983</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.062901</td>\n",
       "      <td>0.204945</td>\n",
       "      <td>0.131422</td>\n",
       "      <td>0.249978</td>\n",
       "      <td>0.118556</td>\n",
       "      <td>1.312690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.123758</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.332386</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.060051</td>\n",
       "      <td>0.174115</td>\n",
       "      <td>0.129949</td>\n",
       "      <td>0.236967</td>\n",
       "      <td>0.107017</td>\n",
       "      <td>1.096409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.128469</td>\n",
       "      <td>0.044693</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>1.012019</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>5.468750</td>\n",
       "      <td>5.382812</td>\n",
       "      <td>0.304910</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.057910</td>\n",
       "      <td>0.156266</td>\n",
       "      <td>0.116783</td>\n",
       "      <td>0.216326</td>\n",
       "      <td>0.099543</td>\n",
       "      <td>1.386837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.109720</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.228795</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.306777</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  X  meanfreq        sd    median       Q25  \\\n",
       "0             0           1  1  0.181338  0.060495  0.187476  0.126197   \n",
       "1             1           2  2  0.186897  0.062260  0.195070  0.130847   \n",
       "2             2           3  3  0.189102  0.062901  0.204945  0.131422   \n",
       "3             4           5  5  0.183036  0.060051  0.174115  0.129949   \n",
       "4             5           6  6  0.168793  0.057910  0.156266  0.116783   \n",
       "\n",
       "        Q75       IQR      skew  ...  centroid   meanfun    minfun    maxfun  \\\n",
       "0  0.233586  0.107389  0.869088  ...  0.181338  0.137742  0.023022  0.271186   \n",
       "1  0.243987  0.113140  1.191767  ...  0.186897  0.121811  0.018412  0.271186   \n",
       "2  0.249978  0.118556  1.312690  ...  0.189102  0.123758  0.083333  0.262295   \n",
       "3  0.236967  0.107017  1.096409  ...  0.183036  0.128469  0.044693  0.258065   \n",
       "4  0.216326  0.099543  1.386837  ...  0.168793  0.109720  0.022472  0.235294   \n",
       "\n",
       "    meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0  0.777344  0.085938  6.226562  6.140625  0.116586    sad  \n",
       "1  0.930339  0.085938  4.000000  3.914062  0.144983    sad  \n",
       "2  0.332386  0.085938  0.625000  0.539062  0.334783    sad  \n",
       "3  1.012019  0.085938  5.468750  5.382812  0.304910    sad  \n",
       "4  0.228795  0.093750  0.750000  0.656250  0.306777    sad  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('emotions_by_voice_registers.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11221211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 909 entries, 0 to 908\n",
      "Data columns (total 24 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Unnamed: 0.1  909 non-null    int64  \n",
      " 1   Unnamed: 0    909 non-null    int64  \n",
      " 2   X             909 non-null    int64  \n",
      " 3   meanfreq      909 non-null    float64\n",
      " 4   sd            909 non-null    float64\n",
      " 5   median        909 non-null    float64\n",
      " 6   Q25           909 non-null    float64\n",
      " 7   Q75           909 non-null    float64\n",
      " 8   IQR           909 non-null    float64\n",
      " 9   skew          909 non-null    float64\n",
      " 10  kurt          909 non-null    float64\n",
      " 11  sp.ent        909 non-null    float64\n",
      " 12  sfm           909 non-null    float64\n",
      " 13  mode          909 non-null    float64\n",
      " 14  centroid      909 non-null    float64\n",
      " 15  meanfun       909 non-null    float64\n",
      " 16  minfun        909 non-null    float64\n",
      " 17  maxfun        909 non-null    float64\n",
      " 18  meandom       909 non-null    float64\n",
      " 19  mindom        909 non-null    float64\n",
      " 20  maxdom        909 non-null    float64\n",
      " 21  dfrange       909 non-null    float64\n",
      " 22  modindx       909 non-null    float64\n",
      " 23  label         909 non-null    object \n",
      "dtypes: float64(20), int64(3), object(1)\n",
      "memory usage: 170.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65463186",
   "metadata": {},
   "source": [
    "### Eliminación de datos que no son necesarios o son de tipo indíce.\n",
    "Eliminaremos los datos del DataSet que son tipo indíce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "256456ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.060495</td>\n",
       "      <td>0.187476</td>\n",
       "      <td>0.126197</td>\n",
       "      <td>0.233586</td>\n",
       "      <td>0.107389</td>\n",
       "      <td>0.869088</td>\n",
       "      <td>2.863717</td>\n",
       "      <td>0.923566</td>\n",
       "      <td>0.307220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.137742</td>\n",
       "      <td>0.023022</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.777344</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>6.226562</td>\n",
       "      <td>6.140625</td>\n",
       "      <td>0.116586</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.062260</td>\n",
       "      <td>0.195070</td>\n",
       "      <td>0.130847</td>\n",
       "      <td>0.243987</td>\n",
       "      <td>0.113140</td>\n",
       "      <td>1.191767</td>\n",
       "      <td>3.878650</td>\n",
       "      <td>0.918848</td>\n",
       "      <td>0.298859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.121811</td>\n",
       "      <td>0.018412</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.930339</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.914062</td>\n",
       "      <td>0.144983</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.062901</td>\n",
       "      <td>0.204945</td>\n",
       "      <td>0.131422</td>\n",
       "      <td>0.249978</td>\n",
       "      <td>0.118556</td>\n",
       "      <td>1.312690</td>\n",
       "      <td>4.589995</td>\n",
       "      <td>0.919519</td>\n",
       "      <td>0.313069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.123758</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.332386</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.060051</td>\n",
       "      <td>0.174115</td>\n",
       "      <td>0.129949</td>\n",
       "      <td>0.236967</td>\n",
       "      <td>0.107017</td>\n",
       "      <td>1.096409</td>\n",
       "      <td>3.680995</td>\n",
       "      <td>0.921361</td>\n",
       "      <td>0.329295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.128469</td>\n",
       "      <td>0.044693</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>1.012019</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>5.468750</td>\n",
       "      <td>5.382812</td>\n",
       "      <td>0.304910</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.057910</td>\n",
       "      <td>0.156266</td>\n",
       "      <td>0.116783</td>\n",
       "      <td>0.216326</td>\n",
       "      <td>0.099543</td>\n",
       "      <td>1.386837</td>\n",
       "      <td>5.031744</td>\n",
       "      <td>0.926238</td>\n",
       "      <td>0.337047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.109720</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.228795</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.306777</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   meanfreq        sd    median       Q25       Q75       IQR      skew  \\\n",
       "0  0.181338  0.060495  0.187476  0.126197  0.233586  0.107389  0.869088   \n",
       "1  0.186897  0.062260  0.195070  0.130847  0.243987  0.113140  1.191767   \n",
       "2  0.189102  0.062901  0.204945  0.131422  0.249978  0.118556  1.312690   \n",
       "3  0.183036  0.060051  0.174115  0.129949  0.236967  0.107017  1.096409   \n",
       "4  0.168793  0.057910  0.156266  0.116783  0.216326  0.099543  1.386837   \n",
       "\n",
       "       kurt    sp.ent       sfm  ...  centroid   meanfun    minfun    maxfun  \\\n",
       "0  2.863717  0.923566  0.307220  ...  0.181338  0.137742  0.023022  0.271186   \n",
       "1  3.878650  0.918848  0.298859  ...  0.186897  0.121811  0.018412  0.271186   \n",
       "2  4.589995  0.919519  0.313069  ...  0.189102  0.123758  0.083333  0.262295   \n",
       "3  3.680995  0.921361  0.329295  ...  0.183036  0.128469  0.044693  0.258065   \n",
       "4  5.031744  0.926238  0.337047  ...  0.168793  0.109720  0.022472  0.235294   \n",
       "\n",
       "    meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0  0.777344  0.085938  6.226562  6.140625  0.116586    sad  \n",
       "1  0.930339  0.085938  4.000000  3.914062  0.144983    sad  \n",
       "2  0.332386  0.085938  0.625000  0.539062  0.334783    sad  \n",
       "3  1.012019  0.085938  5.468750  5.382812  0.304910    sad  \n",
       "4  0.228795  0.093750  0.750000  0.656250  0.306777    sad  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns = [ 'Unnamed: 0.1', 'Unnamed: 0', 'X'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11554258",
   "metadata": {},
   "source": [
    "**Dividimos en dos, el primer conjuntos Y es el conjunto con la etiqueta y el segundo es el conjunto X que va a ser el conjunto de las variables independientes sin la etiqueta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c545914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label']\n",
    "y = pd.get_dummies( y, drop_first = True)\n",
    "X = df.drop(columns =['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d89ab6",
   "metadata": {},
   "source": [
    "### Crear los conjuntod de Train, Validación y Test.\n",
    "A partir de la función *train_test_split* vamos a realizarla dos veces, la primera para dividir en test y train. La segunda para ese train anterior dividirlo en val y train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "347d8f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test, y_train1, y_test = train_test_split( X, y, test_size=0.20, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split( X_train1, y_train1, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b6ee0",
   "metadata": {},
   "source": [
    "## Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1438e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset():\n",
    " \n",
    "  def __init__(self,x,y):\n",
    "    y=y.values\n",
    "    X=x.values\n",
    "    self.X=torch.tensor(X,dtype=torch.float32)\n",
    "    self.y=torch.tensor(y,dtype=torch.float32)\n",
    " \n",
    "  def __len__(self):\n",
    "    return len(self.y)\n",
    "   \n",
    "  def __getitem__(self,idx):\n",
    "    return self.X[idx],self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fc3a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sec=MyDataset(X_train,y_train)\n",
    "test_sec=MyDataset(X_test,y_test)\n",
    "val_sec=MyDataset(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44ecbc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=DataLoader(\n",
    "    train_sec,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    " )\n",
    "\n",
    "test_data=DataLoader(\n",
    "    test_sec,\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=None,\n",
    "    pin_memory=False,\n",
    " )\n",
    "\n",
    "val_data=DataLoader(\n",
    "    val_sec,\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=None,\n",
    "    pin_memory=False,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcc39dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20]) torch.Size([3, 2])\n",
      "tensor([[0.2276, 0.0391, 0.2321, 0.2119, 0.2580, 0.0461, 2.3979, 9.7464, 0.8642,\n",
      "         0.1852, 0.2336, 0.2276, 0.1917, 0.0584, 0.2759, 0.8516, 0.1484, 4.8438,\n",
      "         4.6953, 0.1747],\n",
      "        [0.2048, 0.0441, 0.2082, 0.1876, 0.2297, 0.0421, 1.9322, 6.3588, 0.8904,\n",
      "         0.3525, 0.2066, 0.2048, 0.1781, 0.0343, 0.2462, 0.6719, 0.0781, 4.4219,\n",
      "         4.3438, 0.1271],\n",
      "        [0.1672, 0.0607, 0.1612, 0.1167, 0.2118, 0.0951, 1.0096, 3.4679, 0.9367,\n",
      "         0.4687, 0.0938, 0.1672, 0.1152, 0.0208, 0.2222, 0.5907, 0.0859, 5.3203,\n",
      "         5.2344, 0.1471]]) tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "for i, (data, labels) in enumerate(test_data):\n",
    "  print(data.shape, labels.shape)\n",
    "  print(data,labels)\n",
    "  break;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f16d8fe",
   "metadata": {},
   "source": [
    "## Definir la clase NET \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d672f5a6",
   "metadata": {},
   "source": [
    "Se genera la clase NET para construir la red neuronal. Urilizamos 2 capas ocultas para dar mayor adaptabilidad al patron de los datos. Los nodos se establecieron como una reduccion uniforme desde la cantidad de entrada hasta la cantidad de salida. Como funcion de activacion tomamos la tangente hipebolica con el fin de tener una convergencia mas suave alejandonos de casos de sobre entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27a29ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(20,14)\n",
    "        self.fc2 = nn.Linear(14,8)\n",
    "        self.fc3 = nn.Linear(8,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1790990c",
   "metadata": {},
   "source": [
    "Para el aprendizaje y evaluacion del modelo se utilizaran las funciones de decenso del gradiente y validacion cruzada en entropia. Se encontro que el decenso del gradiente es la mejor funcion para realizar un modelo de clasificación, ya que toma entre todas las probalidades, la que es mayor y a esta la clasifica como 1 y a las otras como 0, hablando de las etiquetas en forma Dummies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73bf18be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Net()\n",
    "\n",
    "optimizer= torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "criterion= nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca030d36",
   "metadata": {},
   "source": [
    "**Entrenar el modelo e ir imprimiendo los errores del test y la validación.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0bccb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,optimizer,loss_module,train_loader,valid_loader,num_epochs):\n",
    "  \n",
    "  valid_loss_min =np.inf  #Vamos a encontrar el menor valor de error de validación. Por eso la inicializmaos como 'infinito'\n",
    "  \n",
    "  for i in range(num_epochs):\n",
    "    model.train()  #ponemos el modelo en modo entrenamiento. Es importante en otras arquitecturas como redes convolucionales.\n",
    "    train_loss = 0.0\n",
    "    v_loss = 0.0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "         # Reiniciar los gradientes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass: calcular la salida para los datos de entrada..\n",
    "        preds = model(data)\n",
    "        #preds = preds.squeeze(dim=1)\n",
    "\n",
    "        # calculate the batch loss\n",
    "        loss = loss_module(preds, target)\n",
    "        \n",
    "        # backpropagation: cálculo de gradientes\n",
    "        loss.backward()\n",
    "   \n",
    "        # actualizar los parámetros\n",
    "        optimizer.step()\n",
    "\n",
    "        # actualizar la cuenta de costos a lo largo de los lotes\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    # for data,labels in testloader:\n",
    "\n",
    "    train_loss = train_loss/len(train_loader.dataset) \n",
    "\n",
    "    model.eval() #Ponemos el modelo en modo evaluación.\n",
    "    \n",
    "    for data,target in valid_loader:\n",
    "      output=model(data)\n",
    "      valid_loss= criterion(output, target)\n",
    "      valid_loss += loss.item()*data.size(0)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    \n",
    "    #imprimir estadísticas de entrenamiento y validación\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        i, train_loss, valid_loss))\n",
    "    \n",
    "\n",
    "    #Guardamos el modelo con el menor error de validación.\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_bikeshare.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3f68058",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7ac0d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 0.477713 \tValidation Loss: 0.002272\n",
      "Validation loss decreased (inf --> 0.002272).  Saving model ...\n",
      "Epoch: 1 \tTraining Loss: 0.475475 \tValidation Loss: 0.002252\n",
      "Validation loss decreased (0.002272 --> 0.002252).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.475337 \tValidation Loss: 0.002262\n",
      "Epoch: 3 \tTraining Loss: 0.475950 \tValidation Loss: 0.002259\n",
      "Epoch: 4 \tTraining Loss: 0.475648 \tValidation Loss: 0.002273\n",
      "Epoch: 5 \tTraining Loss: 0.475251 \tValidation Loss: 0.002265\n",
      "Epoch: 6 \tTraining Loss: 0.474131 \tValidation Loss: 0.002277\n",
      "Epoch: 7 \tTraining Loss: 0.475402 \tValidation Loss: 0.002273\n",
      "Epoch: 8 \tTraining Loss: 0.475812 \tValidation Loss: 0.002261\n",
      "Epoch: 9 \tTraining Loss: 0.475763 \tValidation Loss: 0.002252\n",
      "Validation loss decreased (0.002252 --> 0.002252).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.476045 \tValidation Loss: 0.002244\n",
      "Validation loss decreased (0.002252 --> 0.002244).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.476041 \tValidation Loss: 0.002243\n",
      "Validation loss decreased (0.002244 --> 0.002243).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.475594 \tValidation Loss: 0.002243\n",
      "Epoch: 13 \tTraining Loss: 0.475630 \tValidation Loss: 0.002238\n",
      "Validation loss decreased (0.002243 --> 0.002238).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.475424 \tValidation Loss: 0.002237\n",
      "Validation loss decreased (0.002238 --> 0.002237).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.475423 \tValidation Loss: 0.002236\n",
      "Validation loss decreased (0.002237 --> 0.002236).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.475446 \tValidation Loss: 0.002234\n",
      "Validation loss decreased (0.002236 --> 0.002234).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.475615 \tValidation Loss: 0.002231\n",
      "Validation loss decreased (0.002234 --> 0.002231).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.475746 \tValidation Loss: 0.002227\n",
      "Validation loss decreased (0.002231 --> 0.002227).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.475754 \tValidation Loss: 0.002224\n",
      "Validation loss decreased (0.002227 --> 0.002224).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.475739 \tValidation Loss: 0.002220\n",
      "Validation loss decreased (0.002224 --> 0.002220).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.475728 \tValidation Loss: 0.002217\n",
      "Validation loss decreased (0.002220 --> 0.002217).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.475720 \tValidation Loss: 0.002215\n",
      "Validation loss decreased (0.002217 --> 0.002215).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.475733 \tValidation Loss: 0.002212\n",
      "Validation loss decreased (0.002215 --> 0.002212).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.475753 \tValidation Loss: 0.002211\n",
      "Validation loss decreased (0.002212 --> 0.002211).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.475534 \tValidation Loss: 0.002209\n",
      "Validation loss decreased (0.002211 --> 0.002209).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.475204 \tValidation Loss: 0.002209\n",
      "Validation loss decreased (0.002209 --> 0.002209).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.475103 \tValidation Loss: 0.002200\n",
      "Validation loss decreased (0.002209 --> 0.002200).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.475123 \tValidation Loss: 0.002211\n",
      "Epoch: 29 \tTraining Loss: 0.475285 \tValidation Loss: 0.002186\n",
      "Validation loss decreased (0.002200 --> 0.002186).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.475056 \tValidation Loss: 0.002176\n",
      "Validation loss decreased (0.002186 --> 0.002176).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.475200 \tValidation Loss: 0.002167\n",
      "Validation loss decreased (0.002176 --> 0.002167).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.474777 \tValidation Loss: 0.002171\n",
      "Epoch: 33 \tTraining Loss: 0.474730 \tValidation Loss: 0.002161\n",
      "Validation loss decreased (0.002167 --> 0.002161).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.474623 \tValidation Loss: 0.002170\n",
      "Epoch: 35 \tTraining Loss: 0.474644 \tValidation Loss: 0.002154\n",
      "Validation loss decreased (0.002161 --> 0.002154).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.474420 \tValidation Loss: 0.002156\n",
      "Epoch: 37 \tTraining Loss: 0.474406 \tValidation Loss: 0.002147\n",
      "Validation loss decreased (0.002154 --> 0.002147).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.474480 \tValidation Loss: 0.002144\n",
      "Validation loss decreased (0.002147 --> 0.002144).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.474713 \tValidation Loss: 0.002159\n",
      "Epoch: 40 \tTraining Loss: 0.474260 \tValidation Loss: 0.002141\n",
      "Validation loss decreased (0.002144 --> 0.002141).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 0.474644 \tValidation Loss: 0.002142\n",
      "Epoch: 42 \tTraining Loss: 0.474092 \tValidation Loss: 0.002127\n",
      "Validation loss decreased (0.002141 --> 0.002127).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 0.473811 \tValidation Loss: 0.002128\n",
      "Epoch: 44 \tTraining Loss: 0.473915 \tValidation Loss: 0.002127\n",
      "Validation loss decreased (0.002127 --> 0.002127).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 0.473953 \tValidation Loss: 0.002135\n",
      "Epoch: 46 \tTraining Loss: 0.473816 \tValidation Loss: 0.002122\n",
      "Validation loss decreased (0.002127 --> 0.002122).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.473861 \tValidation Loss: 0.002136\n",
      "Epoch: 48 \tTraining Loss: 0.473928 \tValidation Loss: 0.002130\n",
      "Epoch: 49 \tTraining Loss: 0.473849 \tValidation Loss: 0.002149\n"
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer,criterion,train_data,val_data,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb1c048",
   "metadata": {},
   "source": [
    "**Visualizamos los parametros del mejor modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "208e99b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight tensor([[ 2.1766e-01, -1.7735e-01, -1.2511e-01,  1.4511e-01,  5.1819e-02,\n",
      "         -7.2435e-02, -1.4286e-02, -2.3034e-01, -3.9209e-03,  4.5994e-02,\n",
      "         -2.9191e-02, -4.3357e-02, -1.8438e-01,  1.7918e-01,  1.7249e-01,\n",
      "         -1.4886e-01, -1.5874e-01,  1.7823e-01,  2.4025e-01,  1.7830e-01],\n",
      "        [-4.7940e-02,  1.9522e-01,  1.9599e-01,  1.7656e-01, -1.4781e-01,\n",
      "          1.6202e-01,  2.3718e-02,  1.9442e-01,  8.5202e-03, -8.3254e-02,\n",
      "          1.3728e-01,  4.5471e-02, -1.4416e-01, -2.3577e-01, -2.0084e-01,\n",
      "         -8.3225e-02,  2.5269e-01, -1.3400e-01, -2.3557e-01, -1.6708e-01],\n",
      "        [-3.3434e-02, -2.7957e-02, -2.3445e-01,  3.6040e-02,  1.1874e-01,\n",
      "          1.9986e-01,  1.0432e-01, -3.0392e-01, -1.4393e-01, -8.9026e-02,\n",
      "          8.4492e-02,  1.3470e-01, -8.9885e-02,  1.2328e-01,  8.6457e-02,\n",
      "         -2.9868e-01, -8.1314e-02,  4.3054e-02,  3.5074e-01,  1.3993e-03],\n",
      "        [ 3.4268e-02, -2.4955e-02,  7.2605e-02,  1.0129e-01,  1.0491e-01,\n",
      "          1.7218e-01,  1.6141e-01,  2.8161e-01, -2.9121e-01, -1.8364e-01,\n",
      "         -8.4943e-02,  2.0402e-01,  8.9022e-02,  1.1780e-01,  2.6560e-02,\n",
      "         -2.5238e-03,  3.1053e-01, -1.9033e-01, -3.0670e-01,  1.9052e-01],\n",
      "        [ 3.8729e-02, -6.5641e-02,  7.5597e-02,  1.6496e-01, -1.5443e-01,\n",
      "         -1.3800e-01,  1.1133e-01, -2.6896e-01, -1.3611e-01,  1.5472e-01,\n",
      "         -2.2274e-01, -2.0462e-01, -2.3827e-01, -1.6179e-01,  3.2796e-02,\n",
      "         -1.7096e-01, -8.9911e-02,  2.2212e-02,  4.3832e-01,  2.1606e-02],\n",
      "        [-6.3115e-03, -2.2683e-01, -9.2292e-02,  2.1350e-01, -1.4677e-01,\n",
      "          5.9910e-02,  3.2991e-02, -2.5393e-01, -8.2713e-02,  9.3034e-02,\n",
      "          1.1619e-01, -1.7075e-02, -7.2044e-02,  2.1177e-01, -3.9733e-02,\n",
      "          1.2130e-01, -2.3431e-01,  1.8713e-01,  2.7288e-01, -2.0593e-01],\n",
      "        [-3.2788e-02, -1.8749e-01,  1.0938e-01, -1.0628e-01, -1.4777e-01,\n",
      "         -4.5817e-03, -7.6500e-02,  2.4481e-01,  1.5243e-01,  3.1588e-02,\n",
      "          2.3191e-01,  1.1920e-01, -1.5109e-01,  7.5774e-02, -1.4189e-01,\n",
      "          1.7592e-01,  4.2715e-02, -5.3276e-02, -3.5609e-01,  1.8567e-01],\n",
      "        [-1.0528e-01,  9.8035e-02, -1.9122e-01,  3.1582e-02, -2.6076e-01,\n",
      "         -2.7650e-01,  4.3352e-02,  1.2305e+00, -8.5861e-01, -4.5448e-01,\n",
      "         -6.5912e-02,  7.0451e-02,  2.1459e-01, -1.5617e-01,  1.0282e-01,\n",
      "          4.2248e-01,  4.7354e-02, -6.3075e-01, -8.5147e-01,  5.3786e-02],\n",
      "        [-7.6839e-02,  6.9239e-02, -2.3492e-02,  4.4962e-03,  1.4088e-01,\n",
      "         -2.2408e-01,  1.5963e-01,  2.4930e-01,  4.6292e-02, -8.1828e-02,\n",
      "          1.4373e-01,  1.2457e-01, -1.4956e-01, -2.3994e-01,  4.3802e-02,\n",
      "          6.8952e-02,  2.4446e-01, -1.9593e-01, -2.2975e-01,  1.4859e-01],\n",
      "        [ 2.0313e-01, -1.9959e-01,  1.2380e-01, -1.5820e-01,  1.3377e-01,\n",
      "          2.0168e-01,  6.3769e-02, -2.5442e-01,  5.4431e-02, -7.1299e-02,\n",
      "         -2.3317e-01, -1.4358e-01, -1.0862e-01, -1.8380e-01,  1.0825e-01,\n",
      "         -1.4301e-01, -3.1036e-03,  2.8482e-02,  2.8127e-01,  1.2672e-01],\n",
      "        [-2.4996e-02, -2.1018e-01,  8.1595e-02,  2.6298e-01,  5.8751e-02,\n",
      "         -2.7533e-01,  1.9562e-01,  1.2957e+00, -8.9406e-01, -7.6815e-01,\n",
      "         -1.0991e-01, -7.8814e-04,  2.3775e-01, -6.0987e-03, -1.2698e-01,\n",
      "          5.1744e-01,  1.8108e-01, -8.2044e-01, -8.4063e-01,  9.9883e-02],\n",
      "        [-5.4753e-02, -1.4356e-01, -2.1947e-01, -3.8778e-02, -7.7432e-02,\n",
      "         -1.3103e-01,  1.6165e-01,  9.3064e-01, -6.7844e-01, -1.3696e-01,\n",
      "         -1.6787e-01, -9.0230e-02,  1.9336e-01, -7.5139e-02, -6.9081e-02,\n",
      "          3.9916e-01,  3.4677e-01, -2.6637e-01, -8.1861e-01, -3.0208e-02],\n",
      "        [-1.8129e-01,  1.8655e-01, -9.5549e-04, -1.1371e-01, -1.1314e-01,\n",
      "          3.4802e-02,  1.3533e-01,  1.7052e-01, -5.1789e-02,  1.8321e-01,\n",
      "         -2.2506e-01,  1.2049e-01,  1.7933e-01, -1.0015e-01, -2.2036e-01,\n",
      "         -2.1711e-01,  2.2570e-02, -1.7932e-01, -2.0845e-01, -1.4497e-01],\n",
      "        [-1.4607e-01, -2.0174e-01,  1.9580e-01, -1.3874e-01, -8.9866e-02,\n",
      "          1.9777e-01, -1.8384e-01, -3.3128e-01,  3.4290e-01,  4.8666e-02,\n",
      "          1.1542e-01, -1.5583e-01, -1.8004e-01, -1.1817e-01, -1.0608e-01,\n",
      "         -1.0251e-01, -1.8570e-01,  2.2029e-01,  3.0697e-01, -1.0178e-01]])\n",
      "fc1.bias tensor([-0.1366,  0.0271,  0.1907,  0.0014, -0.1395, -0.0054,  0.0737, -0.7846,\n",
      "        -0.2833,  0.1227, -0.9149, -0.3363, -0.2081, -0.0456])\n",
      "fc2.weight tensor([[-0.0498,  0.0632,  0.0089, -0.0413,  0.0203,  0.0032,  0.0186, -0.0672,\n",
      "         -0.0501, -0.0151, -0.2064, -0.0630,  0.0766, -0.0205],\n",
      "        [ 0.0094,  0.0473,  0.0471, -0.0144, -0.0127,  0.0196,  0.0403, -0.0692,\n",
      "         -0.0099,  0.0110,  0.0863,  0.0216, -0.0539,  0.0738],\n",
      "        [-0.0017,  0.0281, -0.0188, -0.1063,  0.0216,  0.0331, -0.0056,  0.0101,\n",
      "          0.0989,  0.0505,  0.0934,  0.0109, -0.0061,  0.0113],\n",
      "        [-0.0536, -0.0138,  0.0125, -0.0704,  0.0055,  0.0494,  0.0195,  0.2463,\n",
      "         -0.0578, -0.0256,  0.1627,  0.0941, -0.0291,  0.0880],\n",
      "        [ 0.0201,  0.0405, -0.0300,  0.0707, -0.0107,  0.0321, -0.0268, -0.0753,\n",
      "         -0.0484, -0.0030,  0.0130, -0.0037,  0.0302,  0.1229],\n",
      "        [-0.0414,  0.0538,  0.0213,  0.0474,  0.0316,  0.0099,  0.0145,  0.0118,\n",
      "          0.0662,  0.0241, -0.1040, -0.0449, -0.0851,  0.0150],\n",
      "        [-0.0728, -0.0623,  0.0040, -0.0668, -0.0255, -0.0078,  0.0104, -0.0011,\n",
      "          0.0419,  0.0037,  0.0027, -0.1151,  0.0267,  0.0978],\n",
      "        [ 0.0323,  0.0308, -0.0073,  0.0891,  0.0481,  0.0023, -0.0413,  0.0867,\n",
      "         -0.0477,  0.0049,  0.1391, -0.0863, -0.0008, -0.0195]])\n",
      "fc2.bias tensor([ 0.1346,  0.0035, -0.0297, -0.1317,  0.0575,  0.0244,  0.0703, -0.0209])\n",
      "fc3.weight tensor([[-0.0239, -0.0115, -0.0373, -0.2290,  0.0276,  0.0919, -0.2280, -0.1400],\n",
      "        [ 0.0689,  0.0115, -0.0545, -0.3437,  0.0712,  0.1249, -0.1747, -0.1911]])\n",
      "fc3.bias tensor([-0.0496, -0.1132])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21f9e885",
   "metadata": {},
   "source": [
    "### Evaluación del modelo utilizando el Test\n",
    "La metrica que utilizamos para la evaluación del test fue el acurracy, lo elaboramos a mano e implementamos una nueva parte con el segundo for en donde replicamos la funcionalidad del croos entropy para tomar el output que tenga la mayor probabilidad como 1 y el que tiene la menor ponerle 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87f95f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing set accuracy of the network is: 94 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total, correct =0,0\n",
    "for i, data in enumerate(test_data, 0):\n",
    "    inputs, labels = data[0], data[1]\n",
    "    \n",
    "    # Poner cada imagen dentro del input\n",
    "    inputs = inputs.view(-1, 20)\n",
    "    \n",
    "    # Pasar las predicciones\n",
    "    outputs = model(inputs)\n",
    "    # tomar la predicción con mayor valor y poner 1 como clasificación y a las otras 0\n",
    "    for i in range(outputs.size(0)):\n",
    "        maxi = torch.max(outputs[i])\n",
    "        outputs[i] = outputs[i]==maxi\n",
    "    # Realizamos manualmente el acurrancy.\n",
    "    total += labels.size(0)\n",
    "    correct += (outputs == labels).sum().item()\n",
    "    \n",
    "print('The testing set accuracy of the network is: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951ebc0a",
   "metadata": {},
   "source": [
    "### Escoger una muestra aleatoria del DataSet\n",
    "Vamos a tomar una muestra aleatoria con la función Sample y utilizar el modelo anterior para poder predecir el semintimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69fd33bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>happy</th>\n",
       "      <th>sad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>0.185348</td>\n",
       "      <td>0.058613</td>\n",
       "      <td>0.184474</td>\n",
       "      <td>0.135526</td>\n",
       "      <td>0.237368</td>\n",
       "      <td>0.101842</td>\n",
       "      <td>2.701628</td>\n",
       "      <td>14.791517</td>\n",
       "      <td>0.919841</td>\n",
       "      <td>0.368962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126355</td>\n",
       "      <td>0.041237</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.605729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.9375</td>\n",
       "      <td>3.9375</td>\n",
       "      <td>0.130385</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     meanfreq        sd    median       Q25       Q75       IQR      skew  \\\n",
       "759  0.185348  0.058613  0.184474  0.135526  0.237368  0.101842  2.701628   \n",
       "\n",
       "          kurt    sp.ent       sfm  ...   meanfun    minfun    maxfun  \\\n",
       "759  14.791517  0.919841  0.368962  ...  0.126355  0.041237  0.231884   \n",
       "\n",
       "      meandom  mindom  maxdom  dfrange   modindx  happy  sad  \n",
       "759  0.605729     0.0  3.9375   3.9375  0.130385      1    0  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.concat([X,y],axis = 1)\n",
    "\n",
    "sample = df_all.sample(1)\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66c48ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_y = df_all[['happy','sad']]\n",
    "sample_x = df_all.drop(columns = ['happy','sad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a4ab41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0.]], grad_fn=<CopySlices>)\n",
      "     happy  sad\n",
      "759      1    0\n"
     ]
    }
   ],
   "source": [
    "#Pasamos nuestros datos sin la muestra para ser preparados.\n",
    "sample_sec=MyDataset(sample_x, sample_y)\n",
    "\n",
    "# Definimos el Sample Data\n",
    "sample_data=DataLoader(\n",
    "    sample_sec,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=None,\n",
    "    pin_memory=False,\n",
    " )\n",
    "\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "# Pasamos el Sample al modelo\n",
    "for data, target in sample_data:\n",
    "    output=model(data)\n",
    "    for i in range(output.size(0)):\n",
    "        maxi = torch.max(output[i])\n",
    "        output[i] = output[i]==maxi\n",
    "# Luego obtenemos la clasificación dada por nuestro modelo y la clasificación que se tenian en los datos.\n",
    "print((output))\n",
    "print((sample[['happy','sad']]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28d13dab",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72dadc22",
   "metadata": {},
   "source": [
    "* Obtuvimos una presición del modelo del 94%, lo cual nos indica que es un buen modelo de clasificación utilizando una red neuronal con 2 capas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
